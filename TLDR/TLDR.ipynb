{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cooli\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "import nltk\n",
    "import gensim\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "punc_regex = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "\n",
    "def strip_punc(corpus):\n",
    "    return punc_regex.sub('', corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT CODE\n",
    "def to_counter(doc):\n",
    "    \"\"\" \n",
    "    Produce word-count of document, removing all punctuation\n",
    "    and removing all punctuation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    doc : str\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    collections.Counter\n",
    "        lower-cased word -> count\"\"\"\n",
    "    return Counter(strip_punc(doc).lower().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT CODE\n",
    "def to_bag(counters):\n",
    "    \"\"\" \n",
    "    [word_counter0, word_counter1, ...] -> sorted list of unique words\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    counters : Iterable[collections.Counter]\n",
    "        An iterable containing {word -> count} counters for respective\n",
    "        documents.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    List[str]\n",
    "        An alphabetically-sorted list of all of the unique words in `counters`\"\"\"\n",
    "    bag = set()\n",
    "    for counter in counters:\n",
    "        bag.update(counter)\n",
    "    return sorted(bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tf(counter, bag):\n",
    "    return np.array([counter[word] for word in bag], dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_idf(bag, counters):\n",
    "    \"\"\" \n",
    "    Given the bag-of-words, and the word-counts for each document, computes\n",
    "    the inverse document-frequency (IDF) for each term in the bag.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    bag : Sequence[str]\n",
    "        Ordered list of words that we care about\n",
    "\n",
    "    counters : Iterable[collections.Counter]\n",
    "        The word -> count mapping for each document.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        An array whose entries correspond to those in `bag`, storing\n",
    "        the IDF for each term `t`: \n",
    "                           log10(N / nt)\n",
    "        Where `N` is the number of documents, and `nt` is the number of \n",
    "        documents in which the term `t` occurs.\n",
    "    \"\"\"\n",
    "    N = len(counters)\n",
    "    nt = [sum(1 if t in counter else 0 for counter in counters) for t in bag]\n",
    "    nt = np.array(nt, dtype=float)\n",
    "    return np.log10(N / nt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tldr(corpus, threshold=10):\n",
    "    \"\"\" Overall function to consolidate a document via tf-idf sorting.\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "            corpus : String\n",
    "                Full text to summarize.\n",
    "                \n",
    "            threshold : int, optional(default=10)\n",
    "                Number of sentences to include in the summary.\n",
    "                If this value exceeds the number of sentences in\n",
    "                the corpus, the entire text will be returned\n",
    "                unchanged.\n",
    "                \n",
    "        Returns\n",
    "        -------\n",
    "            summary : String\n",
    "                Summarized form of initial input text.\n",
    "    \"\"\"\n",
    "    \n",
    "    #Tokenize the corpus into sentences\n",
    "    sentences = nltk.tokenize.sent_tokenize(corpus)\n",
    "    \n",
    "    if(threshold >= len(sentences)):\n",
    "        return corpus\n",
    "    \n",
    "    #Remove punctuation and lower strings for tf-idf counting\n",
    "    sentences_lower = [strip_punc(token).lower() for token in sentences]\n",
    "\n",
    "    #Generate counters for each sentence and bag of words\n",
    "    counters = []\n",
    "    for sent in sentences_lower:\n",
    "        counters.append(to_counter(sent))\n",
    "        \n",
    "    bag = to_bag(counters)\n",
    "    \n",
    "    #Compute tf descriptors for each sentence and idf descriptor\n",
    "    tfs = np.ndarray((len(sentences), len(bag)))\n",
    "    for i, counter in enumerate(counters):\n",
    "        tfs[i,:] = to_tf(counter, bag)\n",
    "        \n",
    "    idf = to_idf(bag, counters)\n",
    "    \n",
    "    #Compute tf-idf scores, summing along each sentence, and re-sort sentence array\n",
    "    tf_idf = tfs * idf\n",
    "    sentence_scores = np.sum(tf_idf, axis=1)\n",
    "    sentence_ids = sorted(np.argsort(sentence_scores)[::-1][:threshold])\n",
    "    \n",
    "    #Guarantee inclusion of first sentence in the document\n",
    "    if 0 not in sentence_ids:\n",
    "        sentence_ids.insert(0, 0)\n",
    "        sentence_ids.pop()\n",
    "    \n",
    "    #Return joined form of all top sentences\n",
    "    return ' '.join(sentences[i] for i in sentence_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"\"\"The internet age has brought unfathomably massive amounts of information to the fingertips of billions – if only we had time to read it. Though our lives have been transformed by ready access to limitless data, we also find ourselves ensnared by information overload. For this reason, automatic text summarization – the task of automatically condensing a piece of text to a shorter version – is becoming increasingly vital.\n",
    "\n",
    "Two types of summarization\n",
    "There are broadly two approaches to automatic text summarization: extractive and abstractive.\n",
    "\n",
    "Extractive approaches select passages from the source text, then arrange them to form a summary. You might think of these approaches as like a highlighter.\n",
    "\n",
    "Abstractive approaches use natural language generation techniques to write novel sentences. By the same analogy, these approaches are like a pen.\n",
    "\n",
    "The great majority of existing approaches to automatic summarization are extractive – mostly because it is much easier to select text than it is to generate text from scratch. For example, if your extractive approach involves selecting and rearranging whole sentences from the source text, you are guaranteed to produce a summary that is grammatical, fairly readable, and related to the source text. These systems (several are available online) can be reasonably successful when applied to mid-length factual text such as news articles and technical documents.\n",
    "\n",
    "On the other hand, the extractive approach is too restrictive to produce human-like summaries – especially of longer, more complex text. Imagine trying to write a Wikipedia-style plot synopsis of a novel – say, Great Expectations – solely by selecting and rearranging sentences from the book. This would be impossible. For one thing, Great Expectations is written in the first person whereas a synopsis should be in the third person. More importantly, condensing whole chapters of action down to a sentence like Pip visits Miss Havisham and falls in love with her adopted daughter Estella requires powerful paraphrasing that is possible only in an abstractive framework.\n",
    "\n",
    "In short: abstractive summarization may be difficult, but it’s essential!\n",
    "\n",
    "Enter Recurrent Neural Networks\n",
    "If you’re unfamiliar with Recurrent Neural Networks or the attention mechanism, check out the excellent tutorials by WildML, Andrej Karpathy and Distill.\n",
    "\n",
    "In the past few years, the Recurrent Neural Network (RNN) – a type of neural network that can perform calculations on sequential data (e.g. sequences of words) – has become the standard approach for many Natural Language Processing tasks. In particular, the sequence-to-sequence model with attention, illustrated below, has become popular for summarization. Let’s step through the diagram!\n",
    "\n",
    "In this example, our source text is a news article that begins Germany emerge victorious in 2-0 win against Argentina on Saturday, and we’re in the process of producing the abstractive summary Germany beat Argentina 2-0. First, the encoder RNN reads in the source text word-by-word, producing a sequence of encoder hidden states. (There are arrows in both directions because our encoder is bidirectional, but that’s not important here).\n",
    "\n",
    "Once the encoder has read the entire source text, the decoder RNN begins to output a sequence of words that should form a summary. On each step, the decoder receives as input the previous word of the summary (on the first step, this is a special <START> token which is the signal to begin writing) and uses it to update the decoder hidden state. This is used to calculate the attention distribution, which is a probability distribution over the words in the source text. Intuitively, the attention distribution tells the network where to look to help it produce the next word. In the diagram above, the decoder has so far produced the first word Germany, and is concentrating on the source words win and victorious in order to generate the next word beat.\n",
    "\n",
    "Next, the attention distribution is used to produce a weighted sum of the encoder hidden states, known as the context vector. The context vector can be regarded as “what has been read from the source text” on this step of the decoder. Finally, the context vector and the decoder hidden state are used to calculate the vocabulary distribution, which is a probability distribution over all the words in a large fixed vocabulary (typically tens or hundreds of thousands of words). The word with the largest probability (on this step, beat) is chosen as output, and the decoder moves on to the next step.\n",
    "\n",
    "The decoder’s ability to freely generate words in any order – including words such as beat that do not appear in the source text – makes the sequence-to-sequence model a potentially powerful solution to abstractive summarization.\n",
    "\n",
    "Two Big Problems\n",
    "Unfortunately, this approach to summarization suffers from two big problems:\n",
    "\n",
    "Problem 1: The summaries sometimes reproduce factual details inaccurately (e.g. Germany beat Argentina 3-2). This is especially common for rare or out-of-vocabulary words such as 2-0.\n",
    "\n",
    "Problem 2: The summaries sometimes repeat themselves (e.g. Germany beat Germany beat Germany beat…)\n",
    "\n",
    "In fact, these problems are common for RNNs in general. As always in deep learning, it’s difficult to explain why the network exhibits any particular behavior. For those who are interested, I offer the following conjectures. If you’re not interested, skip ahead to the solutions.\n",
    "\n",
    "Explanation for Problem 1: The sequence-to-sequence-with-attention model makes it too difficult to copy a word w from the source text. The network must somehow recover the original word after the information has passed through several layers of computation (including mapping w to its word embedding).\n",
    "\n",
    "In particular, if w is a rare word that appeared infrequently during training and therefore has a poor word embedding (i.e. it is clustered with completely unrelated words), then w is, from the perspective of the network, indistinguishable from many other words, thus impossible to reproduce.\n",
    "\n",
    "Even if w has a good word embedding, the network may still have difficulty reproducing the word. For example, RNN summarization systems often replace a name with another name (e.g. Anna → Emily) or a city with another city (e.g. Delhi → Mumbai). This is because the word embeddings for e.g. female names or Indian cities tend to cluster together, which may cause confusion when attempting to reconstruct the original word.\n",
    "\n",
    "In short, this seems like an unnecessarily difficult way to perform a simple operation – copying – that is a fundamental operation in summarization.\n",
    "\n",
    "Explanation for Problem 2: Repetition may be caused by the decoder’s over-reliance on the decoder input (i.e. previous summary word), rather than storing longer-term information in the decoder state. This can be seen by the fact that a single repeated word commonly triggers an endless repetitive cycle. For example, a single substitution error Germany beat Germany leads to the catastrophic Germany beat Germany beat Germany beat…, and not the less-wrong Germany beat Germany 2-0.\n",
    "\n",
    "Easier Copying with Pointer-Generator Networks\n",
    "Our solution for Problem 1 (inaccurate copying) is the pointer-generator network. This is a hybrid network that can choose to copy words from the source via pointing, while retaining the ability to generate words from the fixed vocabulary. Let’s step through the diagram!\n",
    "\n",
    "This diagram shows the third step of the decoder, when we have so far generated the partial summary Germany beat. As before, we calculate an attention distribution and a vocabulary distribution. However, we also calculate the generation probability via the following formula:\n",
    " \n",
    "This formula just says that the probability of producing word ww is equal to the probability of generating it from the vocabulary (multiplied by the generation probability) plus the probability of pointing to it anywhere it appears in the source text (multiplied by the copying probability).\n",
    "\n",
    "Compared to the sequence-to-sequence-with-attention system, the pointer-generator network has several advantages:\n",
    "\n",
    "The pointer-generator network makes it easy to copy words from the source text. The network simply needs to put sufficiently large attention on the relevant word, and make pgen\n",
    "  sufficiently large.\n",
    "The pointer-generator model is even able to copy out-of-vocabulary words from the source text. This is a major bonus, enabling us to handle unseen words while also allowing us to use a smaller vocabulary (which requires less computation and storage space).\n",
    "The pointer-generator model is faster to train, requiring fewer training iterations to achieve the same performance as the sequence-to-sequence attention system.\n",
    "In this way, the pointer-generator network is a best of both worlds, combining both extraction (pointing) and abstraction (generating).\n",
    "\n",
    "Let’s see a comparison of the systems on some real data! We trained and tested our networks on the CNN / Daily Mail dataset, which contains news articles paired with multi-sentence summaries.\n",
    "\n",
    "The example below shows the source text (a news article about rugby) alongside the reference summary that originally accompanied the article, plus the three automatic summaries produced by our three systems. By hovering your cursor over a word from one of the automatic summaries, you can view the attention distribution projected in yellow on the source text. This tells you where the network was “looking” when it produced that word.\n",
    "\n",
    "For the pointer-generator models, the value of the generation probability is also visualized in green. Hovering the cursor over a word from one of those summaries will show you the value of the generation probability for that word.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The internet age has brought unfathomably massive amounts of information to the fingertips of billions – if only we had time to read it. More importantly, condensing whole chapters of action down to a sentence like Pip visits Miss Havisham and falls in love with her adopted daughter Estella requires powerful paraphrasing that is possible only in an abstractive framework. In this example, our source text is a news article that begins Germany emerge victorious in 2-0 win against Argentina on Saturday, and we’re in the process of producing the abstractive summary Germany beat Argentina 2-0. On each step, the decoder receives as input the previous word of the summary (on the first step, this is a special <START> token which is the signal to begin writing) and uses it to update the decoder hidden state. Finally, the context vector and the decoder hidden state are used to calculate the vocabulary distribution, which is a probability distribution over all the words in a large fixed vocabulary (typically tens or hundreds of thousands of words).\n"
     ]
    }
   ],
   "source": [
    "print(tldr(corpus, threshold=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = \"abcd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abcd']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
